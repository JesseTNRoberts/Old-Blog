## Finding a Topic

So, I have been working on chiasm recognition with causal language models. But this has been harder than I anticipated.

The results I have obtained so far using direct in context learning (ICL) on GPT-3 and Direct++ (adjusting the label probability by dividing by the null conditional probability of the label) ICL have been worse than expected. This has lead me to wonder, why?

I have begun reading papers regarding classification with language models.

### Noisy Channel Language Model Prompting for Few-Shot Text Classification

In this paper, the authors introduce a different method for using language models for classification. Specifically, they use a bayes rule transformation to use the probability of the input given the label $P(x_i|c_i)$ rather than using the direct probability of the label $P(c_i|x_i)$ given the input for classification. So, they use $P(x_i|c_i) = \frac{P(c_i|x_i)*P(x_i)}{P(c_i)}$.
