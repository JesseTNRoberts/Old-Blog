## Finding a Topic

So, I have been working on chiasm recognition with causal language models. But this has been harder than I anticipated.

The results I have obtained so far using direct in context learning (ICL) on GPT-3 and Direct++ (adjusting the label probability by dividing by the null conditional probability of the label) ICL have been worse than expected. This has lead me to wonder, why?

I have begun reading papers regarding classification with language models.

### Noisy Channel Language Model Prompting for Few-Shot Text Classification

In this paper, the authors introduce a different method for using language models for classification. Specifically, they use a bayes rule transformation to use the probability of the input given the label $P(x_i|c_i)$ rather than using the direct probability of the label $P(c_i|x_i)$ given the input for classification. Based on $P(c_i|x_i) = \frac{P(x_i|c_i) \cdot P(c_i)}{P(x_i)}$ they use the direct evaluation of the right hand equation probabilities to estimate the label probability. 

This method is referred to as noisy channel modeling. It seems to have better results in a few key situations. First, is when there is class imbalance in the training data. Second, is when the number of demonstrations is small. 

#### Representation of labels from the training data

One point to make is that the number of demonstrations chosen for the experiments is reflective of the "true" distribution of the labels in the training data. That is, if a label rarely occurs in the training examples, then it would occur very few times in the demonstrations. 

> We relax the assumption from prior work of an equal number of training examples per label (Gao et al., 2021; Logan IV et al., 2021), for more realistic and challenging evaluation.

This seems an important choice and raises a question. 

- Is a causal language model effected by distribution discrepancies between the demonstrations and the test set? Another way of saying the same question would be does a language model implicitly mimic frequency of usage of a label? 

This question is not addressed in the paper. 

#### Notes on channel models

There are a few interesting points that are made throughout. 

> It is also worth noting that direct prompt tuning with upsampling matches or outperforms all finetuning and head tuning when pâˆ’ is small.

This means that if the classes are very imbalanced, finetuning is not likely to be as effective as direct (directly using $P(c_i|x_i)$) prompt tuning.

> our experiment with K = Full confirms the finding from Lester et al. (2021) that direct prompt tuning matches the performance of finetuning all parameters of the LM while being much more parameter efficient.

So, if all the training data is used to perform prompt tuning then finetuning is not more competitve. However, the authors point out that in a few shot learning setup, finetuning outperforms all other current methods. However, this also leaves a question. 

- What training method is most appropriate when the number of training examples is small and there is significant class imbalance?

In general, as the number of demonstrations grows, channel models decrease in comparison to direct probability estimation labeling, both of which are outperformed by finetuning the entire model.

#### Interesting future work suggested 

The authors point out that channel modeling is only applicable in a causal language model. Masked language models don't directly generate long sequences of text in training. So, this prevents them from using channel modeling directly. There is a note about a potential method quoted below:

> One recent approach uses a label-conditioning objective (Tam et al., 2021) as a clever way to introduce a channel-like model with existing masked LMs.

The referenced paper is *Improving and simplifying pattern exploiting training*.



### Idea: One difference in the initial chiasma experiment and the larger trials

In the original experiment, GPT-3 did very well at identifying chiasmi. However, in the trials performance was degraded. One key difference in my implementation was that in the initial experiment I allowed the model to classify an example and then the model's classification of that item was part of the demonstrations for all future examples.

The problem with this setup is that it has a token cost of $O(n \cdot m)$ where $n$ is the number of sentences to be classified and $m$ is the average number of tokens in each sentence. 

That being said, the success it had makes me curious about the method in spite of the token cost. 

This reminds me of regret minimization in a vauge way. Probably because it evokes an image of classification based on non-self contradiction. 

### Which is better: Finetuning a smaller LM or using ICL on a large language model

Rather than considering only the accuracy of models, it would be interesting to compare their efficiency. 

- Considering the cost of pretraining as "sunk" and therefore not accounted for, which method of inference would be the pareto optimal? 
- How does the size of language model factor into the pareto efficiency? (what I mean is characterize the pareto efficiency in terms of the number of parameters of the model)


### Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?

Right to it. There's an important line in this paper:

> Razeghi et al. (2022) show that in-context learning performance is highly correlated with term frequencies in the pretraining data.

- Going back to the distribution of labels being "true" rather than uniform from the paper discussed above, it seems that a follow up question is how does discrepancy of frequency between the training set and the pretraining set effect the results? Should distribution in the demonstrations be matched to the term frequencies in the pretraining set or the training set?

The choice of label has been shown to be important though having a label is always better than not. And it has been shown that adjusting for the unconditioned probability of a label (which is directy related to the label's frequency in the pretraining set) as in direct++ ICL improves performance. However, this normalization of the probabilities seems like it may be inferior to correcting the probabilities. 

All this gets to my idea.

- Given some classification, how can we find optimal labels for ICL? 
- What are the properties of the labels relative to the sentences?
- Is label invariant ICL possible?


