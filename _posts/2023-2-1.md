## Reading about MusicLM, considering writing a survey on ICL, and how can we generate long sequences?

I started by reading the paper regarding audioLM. This paper preceeds the MusicLM paper. The method used in audioLM is a hierarchical sequence to sequence model. It is called hierarchical because audio clips are discretized to create a vocabulary. Then, a w2vec-Bert model is trained to do a MLM task. So, this model learns to "understand" audio sequences. They report that the audio recreations made directly by this model are somewhat coarse (?) but it does learn important long term and short term dependencies. So, the recourse was to layer on an additional model which would use the coarse generation as a seed that would then be used to fill in the coarse missing audio. These audio tokens are then used to build the finer audio output. In this manner, the model is a pipeline of 3 distinct models, the first of which is a language model that speaks discretized music.

AudioLM: a Language Modeling Approach to Audio Generation





## Executive Control and working memory in LLM

### ICL - Questions 

One of the questions around large language models is whether they can learn to include and exclude things with executive oversight based on an objective. The most prevalent mechanism for interacting with a language model is prompting. In this way, the prompt acts as a working memory for the objective. 

- What do we know about ICL and how it works?
- What are the requirements for the pretraining task? the representation? 
- How does term frequency in the pretraining, term frequency in demonstration labels, and task to label distribution disjunction affect ICL?
- Can ICL be used to for NLU beyond semantic structure? Can it be used for things which specifically depend on semantic structure? 
- Can ICL infer and apply strict rules?
- What is the status of ICL visualization and prompt based explanation?



### Hierarchical Structure in Text 


#### Long sequences 

Transformers work based on attention to prompt and the output from the decoder. So, given attention can only be placed on the input and output of certain length, how can transformers generate long text and attend to the entirety?



### Attention is all you need... for open loop language generation

Attention provides a mechanism for direct forward generation of text (causal LM) and for forward inference in NLU (MLM). However, it does not provide a mechanism for evaluation of the generated text in light of the objective and revision. This would constitute a type of closed loop control. 

#### Idea: Using an MLM to act as the feedback loop for a CLM in text generation in response to a prompt

MLMs are shown to be more well suited for NLU while CLMs tend to be more well suited for text generation. The idea then is simple, use a CLM for generation and after each generated token blank a number of previous tokens and ask the MLM to fill in the blanks based on the prompt and the generated text so far. Each of the actions (edit and concatentate) could be considered in series (with the MLM sometimes opting to change nothing) or both actions could be considered simultaneously with the chosen action being sampled from the overall logits.   


