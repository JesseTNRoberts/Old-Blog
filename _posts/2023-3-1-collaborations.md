## Potential Collaborations Discussion

From the meeting a few interesting things did result. 

1. First, for new language models to be built at scale, it will likely require a level of evidence that is similar to what is necessary for clinical trials. However, the markers which suggest success at mass scale are not clearly understood. Less so, the markers that suggest new and interesting behavior. 

New architectures can inherit confidence from existing transformer models like chatgpt. However, the question is will a language model that is encoder only or encoder/decoder only do something that is interestingly different from chatgpt?

2. Can a sufficiently trained decoder only transformer model be prompted sufficiently to generate text representative of any genre?

3. Can a decoder only transformer model be passed a prompt sufficiently to implement long term memory?

