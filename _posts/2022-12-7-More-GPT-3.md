## Experimental Questions

- Do transformer based PLMs possess representations which are sufficient for recognition of rhetorical structures?

- What is the optimal set of exemplars for few shot teaching for GPT-2 based system?
- What is the optimal set of exemplars for teaching BERT?
- Which of these perform best as exemplars for GPT-3 (assuming each set is unique). 

- Can language models expand expand the compact rhetorical meaning represented in structures like chiasma? (Good question but may be addressed in a follow up paper)

## Experimental Design

The easy experiment is to use exemplars for few shot teaching the language models. Then use the language model to classify the structures by presenting patterned cloze test questions to the model.

I am not yet certain how to identify the optimal, minimal exemplar subset beyond brute force. 

It is also not clear how one should ask a language model to expand the meaning of a rhetorical structure. This is a good idea. But I'm not sure that I want to address this in the first paper. 

## Well that went badly

So, the first step to the experimental design above was to get some rudimentary GPT-2 based method working to classify chiasma correctly. However, immediately that proved challenging. Actually, it proved to be a total flop. 

And by total flop I mean, it was randomly generating chiasm and not a chiasm. 

Ok. Why?


## Maybe it was a fluke?

In the original experiment that I ran a few days past, I gave 2 unseen examples each of which was a binary classification task. There is a 25% chance that GPT-3 got lucky. That is it may have essentially generated two random binary values and happened to get them both correct. So, I will redo the experiment with expanded testing. 

Training: 
> 
> ban with permit reservation to a permit with ban: chiasm
> 
> citizen consumers rather than consumer citizens: chiasm
> 
> Independence is language and language is independence: chiasm
> 
> rules and laws and that laws and rules: nothing 
> 
> Directive is inapplicable in Denmark , or Denmark is required by the Directive: nothing
> 

Testing:
> 
> would be appropriate if the European political parties took part in national elections , rather than having national political parties take part in European elections: Chiasm
> 
> entrenches a situation in which protected companies can take over unprotected ones , yet unprotected companies cannot take over protected ones: chiasm
> 
> is much better to bring work to people that to take people to work: chiasm
> 
> bringing about famine , famine brings about wars , and wars are bringing about disease , against: nothing
> 
> man is not made for the law , but rather law is made for man: chiasm
> 
> the question of our overall policy on human rights and the relationship between human rights and foreign policy:  nothing


#### Code

````
import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

restart_sequence = "\n"

response = openai.Completion.create(
  model="text-davinci-003",
  prompt="ban with permit reservation to a permit with ban: chiasm\n\ncitizen consumers rather than consumer citizens: chiasm\n\nIndependence is language and language is independence: chiasm\n\nrules and laws and that laws and rules: nothing \n\nDirective is inapplicable in Denmark , or Denmark is required by the Directive: nothing\n\nwould be appropriate if the European political parties took part in national elections , rather than having national political parties take part in European elections: Chiasm\n\nentrenches a situation in which protected companies can take over unprotected ones , yet unprotected companies cannot take over protected ones: chiasm\n\nis much better to bring work to people that to take people to work: chiasm\n\nbringing about famine , famine brings about wars , and wars are bringing about disease , against: nothing\n\nman is not made for the law , but rather law is made for man: chiasm\n\nthe question of our overall policy on human rights and the relationship between human rights and foreign policy:  nothing",
  temperature=0,
  max_tokens=6,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0,
  stop=["\n"]
)
````

### Fairly Conclusive

Ok. Now it is all but proved that GPT-3 can accurately classify text as containing or not containing chiasmus. The probability that the above results could be generated through a random guess is 1.5%. 


## What the heck

And I don't mean that in a rhetorical sense. I mean it. What is going on here? The model hyperparameters are identical. 


### Hypothesis 1

The most obvious explanation is that the number of parameters in GPT-3 is facilitating better, more meaningful representation which is capturing something about the rhetorical structure. 

I suppose this could be ruled out by using a distilled version of GPT-3 to perform the same task. However, I'm not sure that this would be meaningful. While we could say it is not directly the parameter count, it does not rule out the possibility that it is based on the parameter count of the original model when trained (which seems to be the case for transformer PLMs). 


### Hypothesis 2

The second thing that may be the underlying culprit is the enterprise quality of the software available through openAI. It seems that openAI does not have GPT-2 in their API. So, if there are unpublished helper functions that act as guardrails in the openAI interface it won't be feasible to eliminate those from the experiments if GPT-3 remains part of the tests. Which at this point, it will have too as I have not found a viable alternative with preliminary GPT-2 and BERT tests. 


### Hypothesis 3

It is possible that in the openAI playground, the model I have chosen is optimized for classification. If this is the case, it may be possible to perform similar meta-learning (here I am assuming what type of optimization openAI has done) to boost the ability of GPT-2 and BERT based systems in the same application.

#### Experimental evaluation

I have evaluated this as a possibility by running the same expanded test from above using chatGPT. The results are were generally good, though not as good as with GPT-3. 

Specifically, chatGPT categorized each of the inputs correctly with one exception. It did not perceive a chiasm in: 

> entrenches a situation in which protected companies can take over unprotected ones, yet unprotected companies cannot take over protected ones.

So, chatGPT achieved 83% accuracy. Far and away this is a better result than the random guesses from BERT and GPT-2. However, this model is not optimized (according to openAI) for classification. Rather, this model is optimized for dialogue. 

This suggests that the success of GPT-3 and chatGPT are not attributable to classification optimization. Rather, this seems to suggest that the models may capture meaning which is beyond that captured in GPT-2 and BERT based models.

As an interesting note, I also conducted a test in which I presented chiasms to chatGPT without first establishing the pattern used in the test above. In that case, when asked if the sentences contained chiasma, it always claimed the sentence contained a chiasm. And it would typically give reasoning to support its response. The explanation consistently relied on semantics. That is, chatGPT would argue that due to the presence of tokens in one clause being mirrored in another clause, a chiasm was present. However, those were in fact not chiasma. 

#### Follow up questions

Do large language models capture anything beyond semantic information and are they capable of anything more than semantic reasoning? 

Is semantic reasoning sufficient to consider questions that are typically viewed as sub-textual? As an example, is semantic reasoning sufficient to consider an author's intent?

Is it true that chiasma cannot be identified based solely on semantic information? I suppose another way of saying this is, is intent required for a chiasm to exist?






