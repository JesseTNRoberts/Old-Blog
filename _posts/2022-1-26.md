## Channel Modeling and Masked Language Modeling

The question that I am addressing today is: Why can't masked language models make direct use of noisy channel modeling. That is why can't Bayes rule be applied to MLMs?

To apply Bayes rule, we need to know:
1. the probability of the example given the label 
2. The probability of label
3. The probability of the example

So, which of these can't be calculated for a masked language model?

Certainly the unconditional probability of the label can found. If the unconditional probability of the example could be found, then it would be straight forward to also find the probability conditioned upon the label. So, it must be that the probability of the example can't be easily found.

This matches the only information from the paper which said that MLMs don't generate long sequences of text. However, it would seem that if the perplexity of a model can be found for a given text then the probability of sequece could also be found. 

So, how do MLMs measure perplexity? 

### Perplexity is not well defined for MLMs

There we have it. The very notion of perplexity is not well defined for masked language models. Actually, this makes sense. Consider that the only way arrive at a perplexity like value for an MLM is to masked each word and evaluate the probability of the correct word. However, in doing so, the context $c_i$ of each word $w_i$ is unique. This creates a problem because the multiplication of these $ \prod{i} P_i(w_i|c_i) $ does not equal the probability of the sentence $P(S)$.

So, now I need to look into the paper that the authors previously mentioned regarding how one might be able to condition a masked language model to arrive at something like perplexity. 

Also, it is interesting that some regard MLMs as not suitable for text generation. 



