## Channel Modeling and Masked Language Modeling

The question that I am addressing today is: Why can't masked language models make direct use of noisy channel modeling. That is why can't Bayes rule be applied to MLMs?

To apply Bayes rule, we need to know:
1. the probability of the example given the label 
2. The probability of label
3. The probability of the example

So, which of these can't be calculated for a masked language model?

Certainly the unconditional probability of the label can found. If the unconditional probability of the example could be found, then it would be straight forward to also find the probability conditioned upon the label. So, it must be that the probability of the example can't be easily found.

This matches the only information from the paper which said that MLMs don't generate long sequences of text. However, it would seem that if the perplexity of a model can be found for a given text then the probability of sequece could also be found. 
