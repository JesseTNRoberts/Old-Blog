## A Rose by Any Other Name; or, Do labels matter?

I have shown that GPT-3 and chatGPT seem to be able to recognize chiasma in situations in which previous computational systems have failed. I have also discussed the apparent failure of chatGPT to accomplish this without first being prompted via patterns. This raises the question, is GPT-3 using the existing concept of a chiasm to augment its performance or is it a label that is applied solely based on the presented pattern?


### Experiment

One method to evaluate this would be to apply other arbitrary labels and evaluate the effect on performance. One would expect that the label used would have an effect.

A previous experiment has suggested that semantic labels affect reasoning. That particular experiment was evaluating the strategic reasoning embedded in large language models. There, preliminary results showed that by making something appear similar to a prisoner's dilemma, chatGPT would recognize the form and apply the label prisoner's dilemma. Once the label was applied it would consistently recommend staying silent (even if the expected return of that decision was very poor in comparison to confessing). It seemed to be reasoning based on the self-applied label rather than the content.

In other situations where the form of the scenario did not match the prisoner's dilemma (and therefore did not elicit application of the label), the numbers were used to reason and a better response was given. 

So, one interpretation of this is that a model applied label can interfere with content based reasoning. In the opposite scenario, it is possibel that choosing the appropriate label may have an augmentitive effect. 

## Hold on

This is a good idea. Like a really good idea. I will definitely publish this. 
